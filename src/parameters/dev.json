{"NO_REWARD": {"value": 0, "desc": "No-reward label"}, "INITIAL_EXPLORATION": {"value": 0.6, "desc": "Initial value of epsilon in epsilon-greedy exploration"}, "LEARNING_RATE": {"value": 0.00025, "desc": "The learning rate used by RMSProp"}, "NEGATIVE_REWARD": {"value": -1, "desc": "Negative-reward label"}, "POSITIVE_REWARD": {"value": 1, "desc": "As the scale of scores varies greatly from game to game, we clipped all positive rewards at 1 and all negative rewards at 21, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude."}, "LONG_TERM_MEMORY_SIZE": {"value": 50000, "desc": "SGD updates are sampled from this number of most recent frames"}, "SESSION_SAVE_FILENAME": {"value": "model_", "desc": "Name of the file where the TensorFlow session is saved"}, "MIN_SQUARED_GRADIENT": {"value": 0.01, "desc": "Constant added to the squared gradient in the denominator of the RMSProp update"}, "NO_OP_MAX": {"value": 30, "desc": "Maximum number of do-nothing actions to be performed by the agent at the start of an episode"}, "FRAME_SKIPPING": {"value": 4, "desc": "Following previous approaches to playing Atari 2600 games, we also use a simple frame-skipping technique. More precisely, the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames. Because running the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without significantly increasing the runtime. We use k = 4 for all games."}, "IMAGE_WIDTH": {"value": 84, "desc": "Image width"}, "SESSION_SAVE_DIRECTORY": {"value": "./checkpoints/", "desc": "Directory where the TensorFlow session is saved"}, "MAX_STEPS": {"value": 50000, "desc": "Number of steps before to interrupt the learning procedure"}, "MINIBATCH_SIZE": {"value": 32, "desc": "Number of training cases over which each stochastic gradient descent (SGD) update is computed"}, "DISCOUNT_FACTOR": {"value": 0.99, "desc": "Discount factor gamma used in the Q-learning update"}, "FINAL_EXPLORATION_FRAME": {"value": 1000000, "desc": "The number of frames over which the initial value of epsilon is linearly annealed to its final value"}, "IMAGE_HEIGHT": {"value": 84, "desc": "Image height"}, "FPS": {"value": 30, "desc": "Number of frames per second / frame sampling frequency"}, "desc": "List of hyperparameters retrieved from the original article", "AGENT_HISTORY_LENGTH": {"value": 4, "desc": "The number of most recent frames experienced by the agent that are given as input to the Q network"}, "SHORT_TERM_MEMORY_SIZE": {"value": 200, "desc": "Number of experience samples stored in RAM"}, "UPDATE_FREQUENCY": {"value": 4, "desc": "The number of actions selected by the agent between successive SGD updates. Using a value of 4 results in the agent selecting 4 actions between each pair of successive updates"}, "SQUARED_GRADIENT_MOMENTUM": {"value": 0.95, "desc": "Squared gradient (denominator) momentum used by RMSProp"}, "TARGET_NETWORK_UPDATE_FREQUENCY": {"value": 10000, "desc": "(Default: 10000) The frequency (measured in the number of parameter updates) with which the target network is updated (this corresponds to the parameter C from Algorithm 1)"}, "REPLAY_START_SIZE": {"value": 100, "desc": "(Default: 50.000) A uniform random policy is run for this number of frames before learning starts and the resulting experience is used to populate the replay memory"}, "FINAL_EXPLORATION": {"value": 0.1, "desc": "Final value of epsilon in epsilon-greedy exploration"}, "GRADIENT_MOMENTUM": {"value": 0.95, "desc": "Gradient momentum used by RMSProp"}, "ACTION_REPEAT": {"value": 4, "desc": "Repeat each action selected by the agent this many times. Using a value of 4 results in the agent seeing only every 4th input frame"}}