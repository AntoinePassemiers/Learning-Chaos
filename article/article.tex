\documentclass[letterpaper]{article}
\usepackage{natbib,alifexi}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\newcommand{\E}{\mathbb E}

\title{Human-level control through deep reinforcement learning}
\author{C\'{e}dric Simar, Antoine Passemiers, Stanislas Gueniffey \and Robin Petit \\
\mbox{}\\
Universit\'{e} Libre de Bruxelles \\
apassemi@ulb.ac.be}

\begin{document}
\maketitle

\begin{abstract}

  Reinforcement learning is a subarea of machine learning inspired by neuroscience, where learners have to select actions with
  regards to the environement state and receive a reward accordingly. Each agent's objective is to maximize its cumulative reward across its whole lifetime,
  subdivided into episodes. In the traditional framework of reinforcement learning and more specifically in Q-learning,
  researchers usually deal with a small discrete space to represent the values of Q. This is no longer sufficient if one desires to describe the environment state
  as an image of raw pixels. In consequence, we introduce deep Q-learning methods, which have been found to be really effective in mapping raw pixels
  to abstract high-level features. In deep Q-learning, the estimation of Q-values for a given action are produced using deep neural networks,
  which consist of many neural layers stacked on top of each other.
  These techniques have revealed themselves to be able to beat human experts at playing Atari games
  by learning only from visual features. Despite the current progress, learning to take complex decisions on the basis of high-dimensional visual data remains on
  ongoing challenge. In particular, because of the instability of neural networks, we also explored state-of-the-art dueling architectures to enhance performance
  by separating the high-level representation of states and action values (rewards).

\end{abstract}

\section{Introduction}

We designed the agent in such a way that it selects actions according to its beliefs about future rewards, and its only objective is to maximize its total
cumulative reward. More formally, this cumulative calculation starts from present time $t$ and is computed as follows:
\[
  Q^{*}(s, a) = \max_{\pi} \E\left[ \sum_{k \geq 0}r_{t+k}\gamma^k | s_t = s, a_t = a, \pi\right].
\]
where $\pi = P(a | s)$ is a behavior policy, $t$ is the current step, $r_t$ is the reward for step $t$, $a_t$ is the action selected at time $t$,
and $\gamma$ is the discount factor. Thus, we can described the Q-value as the present value of all future rewards at current step $t$,
with respect to the environment state and the action selected at $t$.

Because of the very high dimensionality of input visual features, it is not reasonable to consider using a table for storing all Q-values.
Instead, one can design a learnable function that maps pixel values to a single row of this table, thus a vector of Q-values whose length is the size
of the action space. For that purpose we used a Deep Q-Network (DQN) to approximate this function, as in the original DQN article \citep{Mnih2015}.
A DQN is a particular instance of Convolutional Neural Networks (CNN) and is composed of a stack of neural layers. One characteristic of CNNs is the
presence of convolutional filters that maps pixel luminance to more abstract features. Each filter (or kernel) is locally connected to its output, which
allows the convolutional layer to capture the local information of the pixels, contrary to what dense (fully-connected) layers do. This procedure is greatly
inspired by the notion of receptive field introduced by Hubel and Wiesel \citep{Hubel1962}.
Let $Q(s, a; \theta_i)$ be the Q-value approximation function represented by the neural network, parameterized by its weights $\theta$ at time step $i$.

Since we are handling visual features, we used screen frames as environment state representations. Each frame is a (210 x 160) color image sampled at
a frequency of 60 Hz. We denote the environment state at current step $t$ by $s_t$. The agent gathers experience by incorporating new experience samples
to its history. An experience is a tuple $e_t = (s_t, a_t, r_t, s_{t+1})$ appended to the experience history immediately after being generated.
To prevent the agent from forgetting the history and increase the bias towards most recent experience samples, we have recourse on the experience
replay algorithm. One can straightforwardly implement it as a uniform sampling algorithm that returns random experience samples at each step.
We compared this technique to a more efficient approach called prioritized experience replay \citep{DBLP:journals/corr/SchaulQAS15}.

  \todo{Antoine suite}

It has been shown that DQN has an ability to learn by acquiring experience generated from policies other than its own.

\section{Methods}

  \todo{pour Robin}

  \todo{Citer ALE} \citep{bellemare13arcade}

\section{Results}

  \todo{Plots avec les erreurs d'apprentissage (pas de cross-val)}

  \todo{Magnifique plot T-SNE avec des couleurs et des gifs et des trucs qui font beep et qui font boop} \citep{wattenberg2016how}

\section{Discussion}

  \todo{Am√©lioration: Double dueling DQN} \citep{DBLP:journals/corr/WangFL15}

\section{Acknowledgements}

  \todo{Remercier Lenaerts, Nowe, Google, OpenAI, Markov}

\footnotesize
\bibliographystyle{apalike}
\bibliography{article}

\end{document}
