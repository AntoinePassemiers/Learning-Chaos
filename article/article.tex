\documentclass[letterpaper]{article}

\usepackage[utf8]{inputenc}
\usepackage[sort, colon]{natbib}
\usepackage{alifexi}
\usepackage{float}
\usepackage{dblfloatfix}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[font=scriptsize]{subcaption}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}

\usepackage[colorlinks=true,citecolor=black,linkcolor=blue]{hyperref}
\usepackage{flushend}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\newcommand{\E}{\mathbb E}

\title{Human-level control through deep reinforcement learning}
\author{C\'{e}dric Simar, Antoine Passemiers, Stanislas Gueniffey \and Robin Petit \\
\mbox{}\\
Universit\'{e} Libre de Bruxelles \\
\{csimar, apassemi, sgueniff, robpetit\}@ulb.ac.be}

\begin{document}
\maketitle

\begin{abstract}

  Reinforcement learning is a subarea of machine learning inspired by neuroscience where learners have to select actions with
  regards to the environment state and receive a reward accordingly. Each agent's objective is to maximize its cumulative reward across its whole lifetime,
  subdivided into episodes. In the traditional framework of reinforcement learning and more specifically in Q-learning,
  researchers usually deal with a small discrete space to represent the values of Q. This is no longer sufficient if one desires to describe the environment state
  as an image of raw pixels. Hence, we introduce deep Q-learning methods, which have been found to be really effective in mapping raw pixels
  to abstract high-level features. In deep Q-learning, the estimation of Q-values for a given action are produced using deep neural networks,
  which consists in many neural layers stacked on top of each other.
  These techniques have proved to be able to beat human experts at playing Atari games
  by learning only from visual features. Despite the current progress, learning how to take complex decisions on the basis of high-dimensional visual data remains an
  ongoing challenge. In particular, because of the instability of neural networks, we also explored state-of-the-art dueling architectures to enhance performance
  by separating the high-level representation of states and action values (rewards).

\end{abstract}

\section{Introduction}

We designed the agent in such a way that it selects actions according to its beliefs about future rewards, and its only objective is to maximize its total
cumulative reward. More formally, this cumulative calculation starts from present time $t$ and is computed as follows:
\begin{equation}
  Q^{*}(s, a) = \max_{\pi} \E\left[ \sum_{k \geq 0}r_{t+k}\gamma^k \; \big| \; s_t = s, a_t = a; \pi\right].
\end{equation}
where $\pi = P(a | s)$ is a behavior policy, $t$ is the current step, $r_t$ is the reward for step $t$, $a_t$ is the action selected at time $t$,
and $\gamma$ is the discount factor. Thus, we can describe the Q-value as the present value of all future rewards at current step $t$,
with respect to the environment state and the action selected at $t$.

Because of the very high dimensionality of input visual features, it is not reasonable to consider using a table for storing all Q-values.
Instead, one can design a learnable function that maps pixel values to a single row of this table, thus a vector of Q-values whose length is the size
of the action space. For that purpose we used a Deep Q-Network (DQN) to approximate this function, as in the original DQN article~\citep{mnih2015human}, since feedforward Neural Networks
are known to be universal function approximators (Theorem 2 in~\cite{hornik1991approximation}).
A DQN is a particular instance of Convolutional Neural Networks (CNN) and is composed of a stack of neural layers~\citep{lecun1998gradient}. One characteristic of CNNs is the
presence of convolutional filters that map pixel luminance to more abstract features. Each filter (or kernel) is locally connected to its output, which
allows the convolutional layer to capture the local information of the pixels, as opposed to what dense (fully-connected) layers do. This procedure is greatly
inspired by the notion of receptive field introduced by Hubel and Wiesel~\citep{Hubel1962}.
Let $Q(s, a; \theta_i)$ be the Q-value approximation function represented by the neural network, parameterized by its weights $\theta$ at time step $i$.

The huge breakout effect of the DQN was that it has shown to be a first step towards general artificial intelligence~\citep{togelius2015ai}, meaning an
algorithm that is not only good at learning one particular task but several of them. Even though the DQN needs to be trained for tens of millions steps
on each Atari game, it gets good at most of them and better than human (considered $75\%$ of human score or more) at 22 of the 49 experimented~\citep{mnih2015human}.

Finally it has been pointed out by the original DQN conceptors that DQN has an ability to learn by acquiring experience generated from policies
other than its own.

One of the big advantages of deep reinforcement learning is that it relies on a general-purpose learning procedure which allows to make
abstraction of each game's specificities. Although this project focuses on performing on Breakout (a popular Atari game),
our agents could be used to learn playing on most Atari game.

\section{Methods}

Our source code is available at \url{https://github.com/RobinPetit/Learning-Chaos}.

We developed our models with the support of the Arcade Learning Environment~\citep{bellemare13arcade}. This gives the opportunity to either
retrieve the environment state as a raw image (for deep reinforcement learning research) or as a state of the RAM (for classic reinforcement learning
applications). Because of the very high dimensionality of raw images, we applied some preprocessing steps on them. These steps are also aimed at
removing some artifacts produced by the Atari architecture. More specifically, the flickering effect implies that some game sprites only appear
in one out of two frames. An easy solution consists in taking the pixelwise maximum value between the current frame and the previous frame.
Then the sample size has been reduced by a factor of three by mapping RGB values to the pixel luminance using the following formula:
\begin{equation}
    \text{Luminance} = 0.299 \times R + 0.587 \times G + 0.114 \times B.
\end{equation}

After mapping the three color channels to a unique channel, the number of pixels has also been reduced. Each input image as been rescaled to a
$84 \times 84$ pixel grid.

Every 16 steps of the Q-learning, the agent builds a minibatch from its memory (either one of the three mechanisms we just mentioned):
this minibatch is used by the DQN to process one step of stochastic gradient descent. In this way the values of Q are estimated iteratively and
refined over time. The deep neural network incorporates minimal prior knowledge about the environment: the only assumption lies in the number of
outputs in the last hidden layer, which has been set as the number of actions. In every Atari game, the action space is discrete, finite and constant over time:
this allows us to determine the output shape of the neural network depending on the game itself. The number of actions made available by the environment
cannot be inferred by the DQN during training phase.

Also we compared DQN, as well as other state-of-the-art deep reinforcement learning architectures, to simpler reinforcement learning techniques (including
random action selection) using a normalized performance metric. The latter is formalized as below:
\begin{equation}
    P = 100 \times (\text{score} - RAS) / (HS - RAS),
\end{equation}
where P designates the normalized performance, RAS the random action selection score, and HS the human score obtained in average by Google's professional
game tester after a training period of 2 hours.  By normalizing in this manner, the random action selection score RAS results in a normalized performance
of 0\% and the human score results in a normalized performance of 100\%.

The perceived rewards of the agent have all been clipped to be either $-1$ for a negative reward, $0$ for a session with no reward, and $+1$
for a positive reward, i.e. at any time $t$: $r_t \in \{-1, 0, +1\}$. No intermediate or larger reward is available to the learning algorithm.
This allows to learn without any previous knowledge of the particular game the DQN is trained on. Yet some rewards can be highly different (in Pac-Man likes,
eating a cherry gives 100 points, a banana gives 5000, and eating a ghost gives $2^n \times 100$ with $n$ the number of eaten ghosts in a single run).
This could be improved by adaptive normalization~\citep{van2016learning}.

As estimating the whole $Q^*$ function is practically not performable, $Q^*$ is estimated by a non-linear function depending on $\theta$ such that:
\begin{equation}
	\forall s \in S : \forall a \in A(s) : Q(s, a; \theta) \simeq Q^*(s, a).
\end{equation}

In this situation, the non-linear function is the neural network, and $\theta$ represents its weights.

The interest of using a neural network is to be able to update the weights such that at each time step $t$, the weights are $\theta_t$, and:
\begin{equation}
	\forall s \in S : \forall a \in A(s) : Q(s, a; \theta_t) \xrightarrow[t \to +\infty]{} Q^*(s, a).
\end{equation}

Therefore, the NN is trained by minimizing the Huber loss~\citep{huber1964robust} with $\delta=1$ of the Bellman equation (by a -- stochastic -- gradient descent).
The Huber loss $H_\delta$, parameterized by $\delta \in {\mathbb R^*}^+$, is a continuous function (also $C^1$ but not $C^2$) defined by the mean
squared error on a small neighbourhood of $0$, and by the absolute value outside this neighbourhood:
\begin{equation}
	H_\delta : \mathbb R \to \mathbb R : x \mapsto \begin{cases}\frac 12x^2                                &\text{ if } \abs x \leq \delta,\\
	                                                            \delta\left(\abs x - \frac 12\delta\right) &\text{ otherwise.}\end{cases}
\end{equation}

The interest of the Huber loss is to bound the gradient of the loss between $-1$ and $+1$, and is claimed to increase stability~\citep{mnih2015human},
yet no further investigation has been performed.

\subsection{Neural Network architecture}

The Deep Q-Network is a Convolutional Neural Network (CNN) composed of three convolutional layers followed by a fully-connected layer,
leading to the final output fully-connected layer.
Between each of these units a non-linear function is applied, which result in Rectifier Linear Units (ReLU) \citep{krizhevsky2012imagenet}, but Exponential Linear Units (ELU) could
also be used since it has been shown to be more performant in classification and decision tasks~\citep{DBLP:journals/corr/ClevertUH15}, but also
Flexible Rectified Linear Units~\citep{qiu2017flexible}.

The first convolutional layer outputs 32 $20 \times 20$ images from a $84 \times 84$ image, using $8 \times 8$ filters and a stride of $4$. The second one outputs
64 images of size $9 \times 9$ from the previous 32, using filters of size $4 \times 4$ and a stride of $2$; and the third and last convolutional layer
applies 64 filters of size $3 \times 3$ with stride $1$ on the 64 subimages of the previous layer to output $7 \times 7$ images.

These are then reshaped into a column vector (of size $7 \times 7 \times 64 = 3136$) and fed into the first fully-connected layer with 512 neurons,
which is then connected to the last layer (again fully-connected) which corresponds to the output with the same size as the action space (4 in the
case of Breakout: \textit{NOOP}, \textit{FIRE}, \textit{LEFT} and \textit{RIGHT}).

In addition to the original DQN article, we also reimplemented a Dueling Deep Q-Network \citep{DBLP:journals/corr/WangFL15} (DDQN) in which the high-level
representation of states
and action values are separated. This shouldn't be confused with the notion of target neural network. Indeed each of our model is copied to a target model
of the same architecture. The DDQN accompanied by its target DDQN, constitute a whole different agent.
For this purpose,the same convolutional architecture as before was kept, but the output of the last convolutional layer was split in two streams of
equal sizes (each stream input size is the same as the output size of the last convolutional layer), then the resulting tensors have been flattened.
The first stream, which is designed to represent the potential advantage per action, is fully-connected to 6 neurons (one per action), while the second stream
contains a dense representation of the state value and is fully-connected to a single neuron.

The last layer of the DDQN outputs Q-value estimates by evaluating the following formula:
\begin{multline}
    Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + \\ \left(A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta, \alpha)\right)
\end{multline}
where $V$ is the output of the state-value stream, $A$ is the output of the advantage-per-action stream, $s$ is the input state (image),
$\alpha$ are the parameters of the state-value stream's fully-connected layer, $\beta$ are the parameters of the state-value stream's
fully-connected layer, $\theta$ are the parameters of the rest of the network, and $\mathcal{A}$ is the discrete action space.

\subsubsection{Implementation}
All CNNs have been implemented using the software library Tensorflow \citep{tensorflow2015-whitepaper}.
Thus our models consist of data flow graphs designed in a symbolic programming fashion, where each node of a graph is an operation
and each edge is a data tensor.

\subsection{Weight update}

In order to reduce oscillation in $Q$ estimations, two separate neural networks have been used: the first one, denoted $Q$ and called the \textit{action-value}
network is the one used in action prediction, and the second one denotes $\hat Q$ and called the \textit{target action-value} network is used when
updating the weights. This particular method holds for both DQN and DDQN The parameters of $Q$ are denoted by $\theta$ as previously, and the parameters of
$\hat Q$ are denoted by $\theta^-$.

Every 4 steps, at time $t$, a minibatch of 32 past experiences $\left\{(s_{t_j}, a_{t_j}, r_{t_j}, s_{t_j+1})\right\}_{j=1}^{32}$ is selected uniformly from the
memory in order to perform a Stochastic Gradient Descent (SGD), thus the average of the gradients of the loss function on each experience is used to update
the weights. As mentioned above, the loss function is a Huber loss function and is defined as:
\begin{equation}
	H_1\left(y_j - Q(s_{t_j}, a_{t_j}; \theta)\right),
\end{equation}
with $y_j$ set either to $r_{t_j}$ if $s_{t_j}$ is a final state (i.e. either the loss of a game or a forced end of episode) or to
$r_{t_j} + \gamma\max_{a' \in A(s_{t_j})}\hat Q(s_{t_j+1}, a'; \theta^-)$ otherwise; therefore using the target action-value network.

The $\hat Q$ target action-value network is then copied back from the action value network $Q$ every 10,000 steps.

\subsection{Hierarchical memory}

Since we are handling visual features, we used screen frames as environment state representations. Each frame is a (210 x 160) color image sampled at
a frequency of 60 Hz. We denote the environment state at current step $t$ by $s_t$. The agent gathers experience by incorporating new experience samples
to its history. An experience is a tuple $e_t = (s_t, a_t, r_t, s_{t+1})$ appended to the experience history immediately after being generated.
To prevent the agent from forgetting the history and increase the bias towards most recent experience samples, we have recourse to the experience
replay algorithm ~\citep{adam2012experience}. One can straightforwardly implement it as a uniform sampling algorithm that returns random experience samples at each step.
We compared this technique to a more efficient approach called prioritized experience replay~\citep{DBLP:journals/corr/SchaulQAS15}.
To emphasize the importance of the memory replay mechanism and as group contribution, we compared the results obtained using prioritized experience replay, uniform
experience replay, and no experience replay.

Regular experience replay consists in feeding the CNN with uniformly random samples stored in memory. To ensure the agent remembers experience samples $e_t$ that
occurred long time ago ($t$ being significantly less than current step), one must allow the replay memory to be really large. Most researchers use a memory size
of 1,000,000 samples, which is too large for most personal computers (over 7 Gb in the framework of our project). Our second and main contribution consisted in
overcoming this issue by implementing a hierarchical style of memory and showing that it does not jeopardize the performance of Deep Q-networks.

Our hierarchical memory consists of a short-term memory and a long-term memory. The latter stores all the experience samples but is not directly accessed by the agent.
Every time the agent needs to perform a mini-batch gradient descent, it requests a mini-batch to the short-term memory. To ensure diversity of the experience,
the short-term memory is updated by copying contiguous data from the long-term memory, where the beginning of the contiguous selected area is selected randomly.
This update operation is repeated every 30,000 steps only in order to avoid a significant reduction in learning speed.

\subsubsection{Implementation}

Long-term experience samples are stored in a large-sized memory-mapped file. The corresponding memory map is used as an abstract data structure that allows to access
the samples by indexing. Short-term memory is implemented as a contiguous array. This array is regularly updated by copying contiguous chunks of data samples from
long-term memory. The agent receives random mini-batches constructed from this short-term array. A contrario, each new experience sample acquired by the agent
(via the environment) is stored directly into the long-term memory. Indeed it appeared that the time required to add a new sample to memory was negligible
compared with the time required to generate a mini-batch of 32 samples. Actually because the random selection is uniform (for regular experience replay),
the selected samples have very large gaps between them, which causes a lot of cache misses.

\subsubsection{Prioritized experience replay}

Prioritized experience replay~\citep{DBLP:journals/corr/SchaulQAS15} has been shown to drastically reduce learning time requirements for many deep
reinforcement learning problems, or even improve model performance in some problems, in particular a large majority of Atari games.
The main shortcoming of uniform experience replay is the fact that mini-batches may contain highly redundant states. Plus, most experience samples have
a zero reward. Gradient descent does not perform well when only negative samples are used. We used rank-based prioritization, which consists in ranking
experience samples by how well they "surprised" the model, and randomly selecting them with respect to weights that are positively correlated with the ranks.

Schaul et al. introduced different ways to compute these ranks. We chose to prioritize with the temporal-difference error:
\begin{multline}
    \delta_t = r_t + \gamma_t Q_{\text{target}}\big(s_t, \argmin_a Q(s_t, a)\big) - Q(s_{t-1}, a_{t-1})
\end{multline}
where $Q_{\text{target}}$ is a function that applies a forward pass using the weights of the target CNN, $Q$ is a similar function
but for the first CNN, and $r_t$ is the reward received at step $t$.

To create a mini-batch, samples are selected according to a weighted distribution, where the weights are computed as follows:
\begin{equation}
    P(j) = p_j^{\alpha} / \sum_i p_i^{\alpha}
\end{equation}
$p_i$ designates the absolute value of the temporal-difference error on particular sample $i$ at current step $t$.
At the start of learning phase the vast majority of probabilities $p_i$ haven't been evaluated yet. This is why we initialized
them with a value of $1$ (which corresponds to a uniform random selection). Coefficient $\alpha$ is a hyper-parameter that
determines how strong the prioritization is. If $\alpha$ is set to zero, one falls back on the case of uniform random selection.

To take into account the relative importance of each sample inside a mini-batch, we had to modify the DQN learning procedure and weight the Huber loss
by an importance sampling weight, calculated from the probabilities $p_i$. For more details about importance sampling weights, please refer to the original
article. The difference between our implementation and what was described in the article is the utilization of the Huber loss instead of only
the temporal-difference error for updating weights.

\section{Results}

\begin{figure*}[!t]
	\vspace{-1.5cm}
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/dqn_uniform_e_scores}
		\subcaption{Total score per episode during a learning phase of 16,000,000 steps, smoothed using a moving average with a window of size 1000.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/dqn_uniform_q_values}
		\subcaption{Estimation of $Q$, averaged across actions, for each batch during a learning phase of 16,000,000 steps and smoothed using a moving
			average with a window of size 5000.}
	\end{subfigure}
	\caption{Episode score and $Q$ values estimation of the DQN after 16,000,000 steps of training on Breakout.\label{fig:dqn uniform}}
\end{figure*}

Figure~\ref{fig:dqn uniform} shows the results of the DQN after 16,000,000 steps of training. One can observe that the first 15,000 batches didn't help
increasing the $Q$ estimates: the internal representation was being built up. Also, the score per episode is very low at first since the agent plays
randomly, a million steps were required for $\epsilon$ to linearly decrease from $1$ to $0.1$.

After this phase, the estimates increase and so does the score per episode, until they both started to settle around 11,000,000 steps. Even though
the score per episode seems to slowly stabilize, the model would need more training time to fully reach stability.

As the $Q$ values computed at each batch are the $Q$ values for the images that the DQN observed at the update step, it is expected to not see
them fully converge: a SGD is performed every 16 steps, independently of the current image. Therefore if the weights update is performed after
a frame where the agent receives positive reward no matter its action (e.g. right before the ball hits a brick) or after a frame where the agent
receives negative reward, no matter his action (e.g. if the ball is about to fall and the paddle is too far away), the $Q$ values will be different.

\begin{table}[!h]
	\centering
	\small
	\setlength\tabcolsep{5pt}
	\begin{tabular}{l|l|l|l|l|l}
		Model & Random & Human & DQN (U) & DQN (P) & DDQN \\ \hline \hline
		Score & $1.31$ & $31.8$ & $39.88$ & $0.22$ & $30.26$ \\ \hline
		P &  $0\%$ & $100\%$ & $126.83\%$ & $-3.57\%$ & $94.95\%$
	\end{tabular}
	\caption{Score and Performance on Breakout (averaged on 100 simulations with $\varepsilon=5\%$ and with one action decision per frame). DQN (U) represents the
	DQN with uniform replay memory, and DQN (P) represents the DQN with prioritized memory. The DQNs have been trained during 16,000,000 steps, and the DDQN has
	been trained for 7,000,000 steps.\label{tab:scores breakout}}
\end{table}

Table~\ref{tab:scores breakout} shows that the DQN performed better than human after 16,000,000 training steps, with a performance of $126.83\%$.

Figure~\ref{fig:t-SNE DQN uniform} shows a 2-dimensional representation of the last fully connected hidden layer, i.e. a 512-dimensional
vector. The method of \textit{projection} that was used is t-SNE~\citep{maaten2008visualizing}, which is part of the manifold learning algorithms, also
called \textit{Nonlinear Dimensionality Reduction}~\citep{lee2007nonlinear}.

The implementation of t-SNE that was used is the one from Scikit-Learn~\citep{scikit-learn}, the Python library. The parameters are the default ones:
a perplexity of 30 and a learning rate of 200 on 1000 iterations. The learning rate is quite high relatively to the propositions in~\cite{wattenberg2016how},
which allows to decrease embedding time. The perplexity lies in the range from 5 to 50 which is advised.

\begin{figure}[!h]
	\includegraphics[width=.5\textwidth]{figures/TSNE_dqn_uniform}
	\caption{Projection of 320,000 states onto a 2-dimensional space. The color of each dot (from dark green to yellow) represents the estimated $Q$ value of the
	best action the agent can play. The states leading to the smallest and the highest estimated $Q$ are displayed with respectively a blue and a red border.
	\label{fig:t-SNE DQN uniform}}
\end{figure}

The states end up clustered by images with similar $Q$ values estimates, so the DQN estimates close $Q$ values for similar images.

The two windows on the right of the figure show the states leading to the smallest estimated $Q$ value (blue) and the highest (red).
We can observe that the image with the smallest estimated $Q$ is an image without ball, meaning that no immediate reward is available. It
is worth noting that the DQN doesn't seem to have learned to use the FIRE action when it misses the ball. The image with the highest estimated
$Q$ is an image where the ball is about to hit a brick, leading to an immediate positive reward.

It is interesting to notice that the image with the smallest $Q$ estimation is an image with no ball rather than an image with a ball about
to be missed by the agent: the DQN considers that, in terms of reward expectation, having missed a ball is worse than missing a ball. This
reinforces the hypothesis that the DQN did not learn to play FIRE since not restarting the game means no reward.

\begin{figure}[!h]
	\includegraphics[width=.5\textwidth]{figures/dqn_uniform_score_distribution}
	\caption{Distribution of the scores performed by the DQN on 100 plays.\label{fig:dqn uniform scores dist}}
\end{figure}

\begin{figure*}[!t]
	\vspace{-1.5cm}
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/ddqn_uniform_e_scores}
		\subcaption{Total score per episode of the DDQN using uniform memory during its learning phase, smoothed using a moving average of size 1000.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/ddqn_uniform_q_values}
		\caption{Estimation of $Q$ by the DDQN using uniform memory, averaged across actions, for each batch during its learning phase and smoothed using a moving average
		with a window of size 5000.}
	\end{subfigure}
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/dqn_prioritized_e_scores}
		\subcaption{Total score per episode of the DQN with prioritized memory during its learning phase, smoothed using a moving average of size 1000.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/dqn_prioritized_q_values}
		\caption{Estimation of $Q$ by the DQN with prioritized memory, averaged across actions, for each batch during its learning phase and smoothed using a moving average
		with a window of size 5000.}
	\end{subfigure}
	\caption{Adaptation of Figure~\ref{fig:dqn uniform} for both DDQN after 16,000,000 training steps (a and b) and DQN with prioritized memory after 7,000,000
	training steps (c and d).\label{fig:DDQN + DQNP}}
\end{figure*}

\begin{figure*}[!t]
	\vspace{-.3cm}
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/ddqn_uniform_score_distribution}
		\caption{Distribution of the score performed by the DDQN with uniform memory on 100 plays.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{.47\textwidth}
		\includegraphics[width=\textwidth]{figures/dqn_prioritized_score_distribution}
		\caption{Distribution of the score performed by the DQN with prioritized memory on 100 plays.}
	\end{subfigure}
	\caption{Adaptation of Figure~\ref{fig:dqn uniform scores dist} for both DDQN (a) and DQN with prioritized memory (b).\label{fig:scores DDQN + DQNP}}
\end{figure*}

Figure~\ref{fig:dqn uniform scores dist} shows the distribution of scores when letting the DQN play for 100 games. We can see that despite having an
average score of 39.88 (with a standard deviation of 41.14), the DQN outputs score ranging from 7 to 219. This means that some states haven't been encountered
enough for the DQN to know significantly which one is better, and can also be slightly related to the $\epsilon$-greedy policy, yet $\epsilon$ is quite small,
and more importantly, the agent selects the action at each frame. This highest score of 219 is probably due to a non-anticipated breaking through
the brick layer, leading to the ball destroying the upper layer whose bricks add 7 points to the score.

Figure~\ref{fig:DDQN + DQNP} shows the evolution of reward per episode and of $Q$ estimates on 16,000,000 steps for the DDQN and 7,000,000 for the DQN with
prioritized memory.

One can observe that the DDQN seems to stabilize around an average reward per episode of 1.4 (though more training time would be needed to ensure convergence),
like the DQN with uniform memory. Yet, $Q$ estimates of the DQN end up around $2$ whereas they are only half as big for the DDQN, which is the very point of
the DDQN: avoid overoptimistic estimations~\citep{DBLP:journals/corr/WangFL15}.

On the other hand, the addition of prioritized memory annihilates the results by making no score at all: Table~\ref{tab:scores breakout} shows that the performance
is negative, so this agent is worse than random; it avoids the ball. Again, this is partly consistent with the results of~\cite{DBLP:journals/corr/SchaulQAS15}
who also had worse performances on Breakout. Yet, the agent should still be able to learn how to play. This deficiency can also be due to the reduction of
the replay memory size compared to the uniform memory or even to the instability of nonlinear estimators of the $Q$ function~\citep{tsitsiklis1996analysis}

The distribution of scores per game for both DDQN and DQN with prioritized memory are shown in Figure~\ref{fig:scores DDQN + DQNP}.

The distribution of the DDQN is very similar to the distribution of the DQN (if ignoring the score of 219 which must be the occurrence
of a very unlikely event), only slightly shifted towards left, leading to the mode being [20, 30] for the DDQN versus [30, 40] for the DQN.

\section{Discussion}

The results obtained in this study don't pretend to outperform the original results by~\cite{mnih2015human} because the timing to develop and train the models,
and the available computing power were restricted, causing the training to not be extensive and preventing from reproducing the learning with different
parameters or even different initializations.

Furthermore, due to the computational cost of training a single model on different networks, we had to select a single game to test, out of the 49 games that were tested
in the original work. The choice of \textit{Breakout} was finally made because of the performance obtained in the original work (over 10 times the score of the
human player), as well as its simplicity in terms of features. Before making any attempt to improve on the described methods and models, one should first apply
them to a wider variety of games, since the strength of this approach lies in its very general domain of application. Still, the efficiency of this method on
different games, and then different types of gameplay, has been proved in~\cite{mnih2015human}.

When comparing scores obtained by the trained network and human players, it should be noted that even though the input available to both kind of players was the same,
the learning process itself is not comparable. While human players learn in great part by applying knowledge from games they previously played, our agent had to
infer every useful feature from scratch. It could be argued that each game taken individually was not designed to be the first and only game played by a human. In fact
some elementary actions such as resuming a game immediately after a life loss, which would be performed instinctively by any human player, were not always performed by
the model, resulting in many useless frames.

Also, other improvements have been performed to this Deep Reinforcement Learning task since~\citep{mnih2015human}.

Asynchronous advantage actor-critic (A3C) replaces the experience replay algorithm described hereabove with an asynchronous and parallel execution of multiple agents,
each of whom evolves within its own, independent instance of the environment. The experience replay's role of decreasing the bias towards the most recent experience samples
is performed by independently using different exploration policies and thus simultaneously experiencing a wide range of states. Moreover, replacing the experience replay
algorithm additionally allows the use on-policy reinforcement learning methods such as actor-critic). The A3C algorithm also tackles the high hardware requirements of
standard DQN algorithms, achieving better results in less training time (almost linear in the number of parallel agents) using a standard multi-core CPU~\citep{mnih2016asynchronous}


\section{Conclusion}

We have implemented a standard DQN as presented in the original paper by Google, and we integrated two different improvements, namely a prioritized memory and a DDQN.
As expected, both the DQN and the DDQN perform at least as well as Google's human tester, but the DQN with prioritized memory failed.

Yet, given the observed results and, the original work, our implementation seems to work, and could be trained on longer period to keep enhancing the results.

\newpage
\footnotesize
\bibliographystyle{apalike}
\bibliography{article}

\end{document}
